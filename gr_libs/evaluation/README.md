# Evaluations
A part of the contribution of this package is standardizing the evaluations of MDP-based GR frameworks.
This sub-package provides scripts to analyze the results of the all_experiments.py execution, done over the ODGR problems defined at consts.py.

The repository provides benchmark domains and scripts for analyzing experimental results. The `evaluation` directory contains tools for processing and visualizing the results from odgr_executor.py and all_experiments.py.

## The evaluation scripts will be added and supported in the future.

# Script: `generate_experiment_results.py`

This script provides a standardized way to visualize and compare the performance of different algorithms (recognizers) on various domains, environments, and tasks, using the results generated by your experimental pipeline (e.g., `all_experiments.py` and `odgr_executor.py`).

## Usage

Do note the paths should be of this format, as outputed by all_experiments.py, since the statistics are done on a large set of executions:
```sh
outputs\GCGraml\parking\Parking-S-14-PC-\L1\experiment_results\res_0.pkl
```

Run the script from the command line with the following arguments:

```bash
python generate_experiments_results.py \
    --domain <domain_name> \
    --env <environment_name> \
    --tasks <task1> <task2> ... <taskN> \
    --recognizers <recognizer1> <recognizer2> ... <recognizerN> \
    --n_runs <number_of_runs> \
    --percentage <observation_percentage> \
    --cons_type <consecutive|non_consecutive> \
    --graph_name <output_figure_base_name>
```

**Example:**
```bash
python generate_experiments_results.py \
    --domain minigrid \
    --env MiniGrid-LavaCrossingS9N2 \
    --tasks L1 L2 L3 L4 L5 \
    --recognizers ExpertBasedGraml Graql \
    --n_runs 5 \
    --percentage 0.7 \
    --cons_type consecutive \
    --graph_name experiment_results
```

- `--domain`: The domain to analyze (e.g., `minigrid`, `parking`).
- `--env`: The environment name within the domain.
- `--tasks`: List of task names (e.g., `L1 L2 L3 L4 L5`).
- `--recognizers`: List of algorithms/recognizers to compare.
- `--n_runs`: Number of experiment runs per task (default: 5).
- `--percentage`: The observation percentage to plot (e.g., `0.7`).
- `--cons_type`: `consecutive` or `non_consecutive` (type of partial observation).
- `--graph_name`: Base name for the output figure (default: `experiment_results`).

## Output

The script generates a PNG file with a plot comparing the mean accuracy (with standard error shading) of each recognizer across the specified tasks. The file will be named like:

```
<graph_name>_<recognizer1>_<recognizer2>_..._<domain>_<env>_<percentage>_<cons_type>.png
```

### Example Output

Below is an example of how the output plot will look (with multiple algorithms, mean lines, and shaded standard error):

![Example Output](example_output.png)

- **Lines**: Each recognizer/algorithm is a line.
- **Shaded Area**: The standard error of the mean (SEM) for each point.
- **X-axis**: Task names.
- **Y-axis**: Accuracy.

## Handling Missing Data

If the script throws an exception like:

```
RuntimeError: No data found for any recognizer in <domain> / <env> / <percentage> / <cons_type>. Missing recognizers: ...
```

This means that the required result files are missing for the specified configuration. To fix this:

1. **Run the experiments** for the missing recognizer/domain/env/task/percentage/type using the `odgr_executor.py` script (or via `all_experiments.py`).
2. Ensure that the result files (e.g., `res_0.pkl`, `res_1.pkl`, ...) are generated in the correct directory structure.
3. Re-run `generate_experiment_results.py` after the data is available.

---

For more details on the experiment pipeline, see the main project README and the documentation for `all_experiments.py` and `odgr_executor.py`.
